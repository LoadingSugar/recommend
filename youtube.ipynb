{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类似于youtube Candidate Generation Model 附上网址 https://zhuanlan.zhihu.com/p/52169807\n",
    "## data： 链接：https://pan.baidu.com/s/1e3onk82FT3IhSrcL-6H_OQ  提取码：ye70 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import linecache\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/dbpedia.train\"\n",
    "test_file = \"data/dbpedia.test\"\n",
    "label_dict = {}\n",
    "sku_dict = {}\n",
    "\n",
    "max_window_size = 1000\n",
    "batch_size = 500\n",
    "emb_size = 128\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "# n_hidden_2 = 256 # 2nd layer number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data(read_file):\n",
    "    #0 is used for padding embedding\n",
    "    label_cnt = 0\n",
    "    sku_cnt = 1\n",
    "    f = open(read_file,'r')\n",
    "    for line in f:\n",
    "        line = line.strip().split(' ')\n",
    "        for i in line:\n",
    "            if i.find('__label__') == 0:\n",
    "                if i not in label_dict:\n",
    "                    label_dict[i] = label_cnt\n",
    "                    label_cnt += 1\n",
    "            else:\n",
    "                if i not in sku_dict:\n",
    "                    sku_dict[i] = sku_cnt\n",
    "                    sku_cnt += 1\n",
    "\n",
    "def read_data(pos, batch_size, data_lst):\n",
    "    batch = data_lst[pos:pos + batch_size]\n",
    "    x = np.zeros((batch_size, max_window_size))\n",
    "    mask = np.zeros((batch_size, max_window_size))\n",
    "    y = []\n",
    "    word_num = np.zeros((batch_size))\n",
    "    line_no = 0\n",
    "    for line in batch:\n",
    "        line = line.strip().split(' ')\n",
    "        y.append(label_dict[line[0]])\n",
    "        col_no = 0\n",
    "        for i in line[1:]:\n",
    "            if i in sku_dict:\n",
    "                x[line_no][col_no] = sku_dict[i]\n",
    "                mask[line_no][col_no] = 1\n",
    "                col_no += 1\n",
    "            if col_no >= max_window_size:\n",
    "                break\n",
    "        word_num[line_no] = col_no\n",
    "        line_no += 1\n",
    "\n",
    "    return x, np.array(y).reshape(batch_size, 1), mask.reshape(batch_size, max_window_size, 1), word_num.reshape(batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Num:  14\n"
     ]
    }
   ],
   "source": [
    "#========================\n",
    "init_data(train_file)\n",
    "n_classes = len(label_dict)\n",
    "train_lst = linecache.getlines(train_file)\n",
    "print(\"Class Num: \", n_classes)\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([emb_size, n_hidden_1])),\n",
    "    # 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    # 'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    #x = tf.nn.dropout(x, 0.8)\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    #dlayer_1 = tf.nn.dropout(layer_1, 0.5)\n",
    "    #layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    #layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    # out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    # return out_layer\n",
    "    return layer_1\n",
    "\n",
    "embedding = {\n",
    "    'input':tf.Variable(tf.random_uniform([len(sku_dict)+1, emb_size], -1.0, 1.0))\n",
    "    # 'output':tf.Variable(tf.random_uniform([len(label_dict)+1, emb_size], -1.0, 1.0))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 14:44:27.652238 140377874786112 deprecation.py:323] From <ipython-input-6-77618e93b8c9>:8: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0811 14:44:27.738372 140377874786112 deprecation.py:323] From /root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "emb_mask = tf.placeholder(tf.float32, shape=[None, max_window_size, 1])\n",
    "word_num = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "x_batch = tf.placeholder(tf.int32, shape=[None, max_window_size])\n",
    "y_batch = tf.placeholder(tf.int64, [None, 1])\n",
    "\n",
    "input_embedding = tf.nn.embedding_lookup(embedding['input'], x_batch)\n",
    "project_embedding = tf.div(tf.reduce_sum(tf.multiply(input_embedding,emb_mask), 1),word_num)# batch*max_window_size*emb_size ==>bath*emb_size\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(project_embedding, weights, biases)#bath*emb_size  (dot) emb_size*n_hidden_1 +biases ==>bath*n_hidden_1\n",
    "\n",
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_classes, n_hidden_1],\n",
    "                        stddev=1.0 / math.sqrt(n_hidden_1)))\n",
    "nce_biases = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=y_batch,\n",
    "                     inputs=pred,\n",
    "                     num_sampled=10,\n",
    "                     num_classes=n_classes))\n",
    "\n",
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "out_layer = tf.matmul(pred, tf.transpose(nce_weights)) + nce_biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_batch of training data:  1120\n",
      "Epoch: 0001 cost= 0.002873009\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.974\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.966\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.966\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.974\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.974\n",
      "Batch Accuracy:  0.992\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.974\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.966\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.974\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.97\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.99\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.968\n",
      "Batch Accuracy:  0.988\n",
      "Batch Accuracy:  0.994\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.97\n",
      "Batch Accuracy:  0.996\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.976\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.968\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.978\n",
      "Batch Accuracy:  0.972\n",
      "Batch Accuracy:  0.984\n",
      "Batch Accuracy:  0.968\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.97\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.98\n",
      "Batch Accuracy:  0.982\n",
      "Batch Accuracy:  0.968\n",
      "Batch Accuracy:  0.986\n",
      "Batch Accuracy:  0.986\n",
      "Final Accuracy:  0.9808571474892752\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    start_time = time.time()\n",
    "    total_batch = int(len(train_lst) / batch_size)\n",
    "    print(\"total_batch of training data: \", total_batch)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        for i in range(total_batch):\n",
    "            x, y, batch_mask, word_number = read_data(i * batch_size, batch_size, train_lst)\n",
    "            _,c = sess.run([optimizer, cost], feed_dict={x_batch: x, emb_mask: batch_mask, word_num: word_number, y_batch: y})\n",
    "            #print(\"Epoch %d Batch %d Elapsed time %fs\" %(epoch, i, time.time() - start_time))\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "            # correct_prediction = tf.equal(tf.argmax(out_layer, 1), tf.reshape(y_batch, [batch_size]))\n",
    "            # accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            # print(\"Accuracy:\", accuracy.eval({x_batch: x, y_batch: y, emb_mask: batch_mask, word_num: word_number}))\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \\\n",
    "              \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(out_layer, 1), tf.reshape(y_batch, [batch_size]))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "    test_lst = linecache.getlines(test_file)\n",
    "    total_batch = int(len(test_lst) / batch_size)\n",
    "    final_accuracy = 0\n",
    "    for i in range(total_batch):\n",
    "        x, y, batch_mask, word_number = read_data(i*batch_size, batch_size, test_lst)\n",
    "        batch_accuracy = accuracy.eval({x_batch: x, y_batch: y, emb_mask: batch_mask, word_num: word_number})\n",
    "        print(\"Batch Accuracy: \", batch_accuracy)\n",
    "        final_accuracy += batch_accuracy\n",
    "    print(\"Final Accuracy: \", final_accuracy * 1.0 / total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding2 = {\n",
    "    'input':tf.Variable(tf.random_uniform([len(sku_dict)+1, emb_size], -1.0, 1.0))\n",
    "    # 'output':tf.Variable(tf.random_uniform([len(label_dict)+1, emb_size], -1.0, 1.0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8758478 ,  0.34019566,  0.082232  ,  0.68539095, -0.3927145 ,\n",
       "        0.7513776 , -0.60025716, -0.47553897, -0.10748911,  0.01165295,\n",
       "        0.8743646 , -0.6310854 , -0.85378003,  0.7571566 , -0.63642955,\n",
       "        0.8241277 ,  0.45026827,  0.25037718, -0.33855772,  0.2109375 ,\n",
       "        0.7913141 ,  0.11828518,  0.43765187,  0.81121707, -0.2500875 ,\n",
       "       -0.05734873, -0.1650126 ,  0.38498878,  0.37742782,  0.9495721 ,\n",
       "       -0.41694784, -0.77679896,  0.8964803 , -0.00870299, -0.1644988 ,\n",
       "        0.12021565,  0.5084343 ,  0.08331561,  0.01648998, -0.7906444 ,\n",
       "        0.18836236,  0.60497046, -0.18774438,  0.5860958 ,  0.72163534,\n",
       "        0.09724617, -0.65918446,  0.7554071 ,  0.58323264,  0.6362982 ,\n",
       "        0.32701492, -0.61844397, -0.20653105, -0.8768456 , -0.90562415,\n",
       "        0.49605894, -0.44254804, -0.99577117,  0.9388304 ,  0.1828866 ,\n",
       "       -0.2825377 , -0.6500766 ,  0.3259263 , -0.08249831, -0.1181612 ,\n",
       "       -0.72467494, -0.5125823 ,  0.36024046, -0.84710145,  0.3499992 ,\n",
       "       -0.18979788, -0.5691955 ,  0.12191701,  0.37113833,  0.00991488,\n",
       "        0.12682366, -0.07002926, -0.04023314,  0.28209877, -0.38940787,\n",
       "       -0.55213785, -0.00680399,  0.9682925 , -0.6295419 , -0.34084177,\n",
       "       -0.47603846, -0.0337131 , -0.33637166,  0.4197569 ,  0.41510105,\n",
       "       -0.61350274, -0.40877056,  0.73795485, -0.94774365, -0.9362209 ,\n",
       "       -0.8363025 ,  0.1373589 ,  0.26206207, -0.60748696,  0.30734277,\n",
       "        0.7465098 ,  0.8226857 ,  0.82071304,  0.36228895,  0.22889018,\n",
       "       -0.92903805,  0.26709104, -0.26537585,  0.43375945,  0.6276107 ,\n",
       "       -0.67001367,  0.10513687, -0.15521073, -0.55854154, -0.6073911 ,\n",
       "       -0.2721181 ,  0.10482764, -0.44183207, -0.46114898, -0.7451942 ,\n",
       "        0.0429666 ,  0.61200356, -0.70371914,  0.06744838,  0.311085  ,\n",
       "        0.9811938 , -0.58908916,  0.00855589], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run(embedding2['input'][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.82630656 0.6855396  0.05022895 0.21493632 0.01809424]\n",
      "  [0.82630656 0.6855396  0.05022895 0.21493632 0.01809424]\n",
      "  [0.82630656 0.6855396  0.05022895 0.21493632 0.01809424]\n",
      "  [0.14551133 0.38358059 0.52439619 0.92029868 0.86354287]]\n",
      "\n",
      " [[0.26615393 0.17822771 0.5855087  0.29269982 0.02404527]\n",
      "  [0.82630656 0.6855396  0.05022895 0.21493632 0.01809424]\n",
      "  [0.14551133 0.38358059 0.52439619 0.92029868 0.86354287]\n",
      "  [0.14551133 0.38358059 0.52439619 0.92029868 0.86354287]]\n",
      "\n",
      " [[0.24434086 0.1200454  0.65374472 0.39212664 0.69070037]\n",
      "  [0.26615393 0.17822771 0.5855087  0.29269982 0.02404527]\n",
      "  [0.82630656 0.6855396  0.05022895 0.21493632 0.01809424]\n",
      "  [0.14551133 0.38358059 0.52439619 0.92029868 0.86354287]]]\n",
      "[[0.14551133 0.38358059 0.52439619 0.92029868 0.86354287]\n",
      " [0.82630656 0.6855396  0.05022895 0.21493632 0.01809424]\n",
      " [0.26615393 0.17822771 0.5855087  0.29269982 0.02404527]\n",
      " [0.24434086 0.1200454  0.65374472 0.39212664 0.69070037]\n",
      " [0.62156907 0.55158928 0.50647869 0.2959347  0.46571578]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "c = np.random.random([5,5])\n",
    "b = tf.nn.embedding_lookup(c, [[1,1,1,0],[2,1,0,0],[3,2,1,0]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(tf.initialize_all_variables())\n",
    "\tprint(sess.run(b))\n",
    "    \n",
    "\tprint(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.55544131, 0.8304293 ],\n",
       "        [0.68217629, 0.95945437],\n",
       "        [0.2687992 , 0.8579725 ],\n",
       "        [0.84239537, 0.66280121]],\n",
       "\n",
       "       [[0.80892266, 0.99986152],\n",
       "        [0.07148699, 0.85757265],\n",
       "        [0.86742756, 0.20367565],\n",
       "        [0.04267489, 0.83615199]],\n",
       "\n",
       "       [[0.45284969, 0.7625072 ],\n",
       "        [0.8613855 , 0.1709657 ],\n",
       "        [0.90207779, 0.2061405 ],\n",
       "        [0.5995104 , 0.8500277 ]],\n",
       "\n",
       "       [[0.57260413, 0.31813009],\n",
       "        [0.52549597, 0.71890025],\n",
       "        [0.08609023, 0.7512856 ],\n",
       "        [0.74849522, 0.62619595]],\n",
       "\n",
       "       [[0.22367012, 0.51006928],\n",
       "        [0.75731616, 0.38349344],\n",
       "        [0.25053126, 0.69025291],\n",
       "        [0.78617543, 0.989679  ]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = np.random.random([5,4,2])\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.34881217 3.31065738]\n",
      " [1.79051211 2.89726181]\n",
      " [2.81582338 1.98964109]\n",
      " [1.93268556 2.41451189]\n",
      " [2.01769298 2.57349463]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hh=tf.reduce_sum(cc,1)\n",
    "nce_biases1 = tf.Variable(tf.zeros([n_classes,2]))\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(tf.initialize_all_variables())\n",
    "\tprint(sess.run(hh))\n",
    "\tprint(sess.run(nce_biases1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.61348792 3.42099739]\n",
      " [2.8978609  3.0903864 ]\n",
      " [2.37492604 2.70932716]\n",
      " [3.01925132 3.96485585]]\n"
     ]
    }
   ],
   "source": [
    "hh=tf.reduce_sum(cc,0)\n",
    "with tf.Session() as sess:\n",
    "\tsess.run(tf.initialize_all_variables())\n",
    "\tprint(sess.run(hh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-1.9546583   0.33155093  0.95034134 -0.08760493  1.6392941 ]]\n",
      "\n",
      "  [[ 0.38250458 -0.07407135 -0.63093686  0.08415447 -1.6335772 ]]]\n",
      "\n",
      "\n",
      " [[[ 1.962625    1.2704955  -1.1871498  -0.41778094  0.7825305 ]]\n",
      "\n",
      "  [[-0.5111942   0.602534   -0.09397011  1.4350804   1.3380942 ]]]]\n"
     ]
    }
   ],
   "source": [
    "c = tf.truncated_normal(shape=[2,2,1,5], mean=0, stddev=1)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "\tprint( sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
